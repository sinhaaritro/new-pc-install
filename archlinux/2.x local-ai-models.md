# Installing Local AI Models

## Install package for running NVIDIA GPU in containers
Install: `sudo pacman -Syu nvidia-container-toolkit`

Check if installed: `nvidia-ctk --version`

## Generate the config file for nvidia-container-toolkit (Needed to execute everytime NVIDIA Driver are updated)
Generate config file: `sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml`

Get List of GPU: `nvidia-ctk cdi list`

## (Optional) Check with upbuntu image if NVIDIA GPU in containers
Get Podman container: `podman pull ubuntu`

Run the container with privileged permissions: `podman run --privileged --rm --gpus all ubuntu nvidia-smi`

You should see the `nvidia-smi` UI with the correct GPU name, and the process should be empty indicating that `nvidia-smi` is running from the container

## Install Ollama
Pull from Podman: `podman pull docker.io/ollama/ollama`

Create a local folder to store the models: `mkdir LocalModels`

Run the container `podman run --name ollama --rm --detach --privileged --gpus all -p 11434:11434 -v $PWD/LocalModels/:root/.ollama ollama/ollama`

Check logs: `podman logs -f ollama`

It should mention the you NVIDIA GPU name

## Create Alias for Ollama in the ZSH
Open the file: `nvim ./.config/zsh/aliases.zsh`

Add the lines

```bash
alias ollama='podman exec ollama ollama' # Alias to start and run the Ollama container

# This function will let you run OLLAMA with Web UI at a custom or default mentioned port RUN: `start-webui` or `start-webui 9000`
function start-webui() {
  local port="${1:-11435}"  # Use the first argument as the port, or default to 11435
  
  echo "Starting Open WebUI on host port: ${port}"
  
  podman run \
    -d \
    --name open-webui \
    --rm \
    -p "${port}:8080" \
    -e OLLAMA_BASE_URL=http://host.containers.internal:11434 \
    -e WEBUI_AUTH=false \
    ghcr.io/open-webui/open-webui:main
}
```

## Run the locally installed Ollama
Run it: `ollama`

## Setup models
Pull the models from Ollama Library: `ollama pull deepseek-r1`

## Setup WebUI for chatting
Get the container: `podman pull ghcr.io/open-webui/open-webui:main`

Run the container: `podman run --name open-webui --rm -p 11435:8080 -e WEBUI_AUTH=false -e OLLAMA_BASE_URL=http://host.containers.internal:11434 ghcr.io/open-webui/open-webui:main`

Open the URL [http://127.0.0.1:11435](http://127.0.0.1:11435/) in browser.




